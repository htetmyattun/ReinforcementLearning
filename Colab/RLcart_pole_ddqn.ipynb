{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RLcart_pole_ddqn.ipynb","provenance":[],"authorship_tag":"ABX9TyM8ka5CMdXzbd+scm7qzSRd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"E2M59I85vaiQ","executionInfo":{"status":"ok","timestamp":1602034963101,"user_tz":-480,"elapsed":3774,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["# Tutorial by www.pylessons.com\n","# Tutorial written for - Tensorflow 1.15, Keras 2.2.4\n","\n","import os\n","import random\n","import gym\n","import pylab\n","import numpy as np\n","from collections import deque\n","from keras.models import Model, load_model\n","from keras.layers import Input, Dense, Lambda, Add\n","from keras.optimizers import Adam, RMSprop\n","from keras import backend as K"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"4dpGHtLKvePW","executionInfo":{"status":"ok","timestamp":1602034963104,"user_tz":-480,"elapsed":3762,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["def OurModel(input_shape, action_space, dueling):\n","    X_input = Input(input_shape)\n","    X = X_input\n","\n","    # 'Dense' is the basic form of a neural network layer\n","    # Input Layer of state size(4) and Hidden Layer with 512 nodes\n","    X = Dense(512, input_shape=input_shape, activation=\"relu\", kernel_initializer='he_uniform')(X)\n","\n","    # Hidden layer with 256 nodes\n","    X = Dense(256, activation=\"relu\", kernel_initializer='he_uniform')(X)\n","    \n","    # Hidden layer with 64 nodes\n","    X = Dense(64, activation=\"relu\", kernel_initializer='he_uniform')(X)\n","\n","    if dueling:\n","        state_value = Dense(1, kernel_initializer='he_uniform')(X)\n","        state_value = Lambda(lambda s: K.expand_dims(s[:, 0], -1), output_shape=(action_space,))(state_value)\n","\n","        action_advantage = Dense(action_space, kernel_initializer='he_uniform')(X)\n","        action_advantage = Lambda(lambda a: a[:, :] - K.mean(a[:, :], keepdims=True), output_shape=(action_space,))(action_advantage)\n","\n","        X = Add()([state_value, action_advantage])\n","    else:\n","        # Output Layer with # of actions: 2 nodes (left, right)\n","        X = Dense(action_space, activation=\"linear\", kernel_initializer='he_uniform')(X)\n","\n","    model = Model(inputs = X_input, outputs = X, name='CartPoleDuelingDDQNmodel')\n","    model.compile(loss=\"mean_squared_error\", optimizer=RMSprop(lr=0.00025, rho=0.95, epsilon=0.01), metrics=[\"accuracy\"])\n","\n","    model.summary()\n","    return model"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNEfOjZnvh6a","executionInfo":{"status":"ok","timestamp":1602034963108,"user_tz":-480,"elapsed":3760,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}},"outputId":"8f3ec1ce-29ae-4d8a-a6fa-96d9fd326b7c","colab":{"base_uri":"https://localhost:8080/","height":36}},"source":["class DQNAgent:\n","    def __init__(self, env_name):\n","        self.env_name = env_name       \n","        self.env = gym.make(env_name)\n","        self.env.seed(0)  \n","        # by default, CartPole-v1 has max episode steps = 500\n","        self.env._max_episode_steps = 4000\n","        self.state_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","\n","        self.EPISODES = 10000\n","        self.memory = deque(maxlen=2000)\n","        \n","        self.gamma = 0.95    # discount rate\n","        self.epsilon = 1.0  # exploration rate\n","        self.epsilon_min = 0.01 # minimum exploration probability\n","        self.epsilon_decay = 0.999 # exponential decay rate for exploration prob\n","        self.batch_size = 32 \n","        self.train_start = 1000\n","\n","        # defining model parameters\n","        self.ddqn = True # use doudle deep q network\n","        self.Soft_Update = False # use soft parameter update\n","        self.dueling = True # use dealing netowrk\n","\n","        self.TAU = 0.1 # target network soft update hyperparameter\n","\n","        #self.Save_Path = 'Models'\n","        #if not os.path.exists(self.Save_Path): os.makedirs(self.Save_Path)\n","        self.scores, self.episodes, self.average = [], [], []\n","        \n","        if self.ddqn:\n","            print(\"----------Double DQN--------\")\n","            #self.Model_name = os.path.join(self.Save_Path,\"Dueling DDQN_\"+self.env_name+\".h5\")\n","        else:\n","            print(\"-------------DQN------------\")\n","            #self.Model_name = os.path.join(self.Save_Path,\"Dueling DQN_\"+self.env_name+\".h5\")\n","        \n","        # create main model and target model\n","        self.model = OurModel(input_shape=(self.state_size,), action_space = self.action_size, dueling = self.dueling)\n","        self.target_model = OurModel(input_shape=(self.state_size,), action_space = self.action_size, dueling = self.dueling)\n","\n","    # after some time interval update the target model to be same with model\n","    def update_target_model(self):\n","        if not self.Soft_Update and self.ddqn:\n","            self.target_model.set_weights(self.model.get_weights())\n","            return\n","        if self.Soft_Update and self.ddqn:\n","            q_model_theta = self.model.get_weights()\n","            target_model_theta = self.target_model.get_weights()\n","            counter = 0\n","            for q_weight, target_weight in zip(q_model_theta, target_model_theta):\n","                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n","                target_model_theta[counter] = target_weight\n","                counter += 1\n","            self.target_model.set_weights(target_model_theta)\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","        if len(self.memory) > self.train_start:\n","            if self.epsilon > self.epsilon_min:\n","                self.epsilon *= self.epsilon_decay\n","\n","    def act(self, state):\n","        if np.random.random() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        else:\n","            return np.argmax(self.model.predict(state))\n","\n","    def replay(self):\n","        if len(self.memory) < self.train_start:\n","            return\n","        # Randomly sample minibatch from the memory\n","        minibatch = random.sample(self.memory, self.batch_size)\n","\n","        state = np.zeros((self.batch_size, self.state_size))\n","        next_state = np.zeros((self.batch_size, self.state_size))\n","        action, reward, done = [], [], []\n","\n","        # do this before prediction\n","        # for speedup, this could be done on the tensor level\n","        # but easier to understand using a loop\n","        for i in range(self.batch_size):\n","            state[i] = minibatch[i][0]\n","            action.append(minibatch[i][1])\n","            reward.append(minibatch[i][2])\n","            next_state[i] = minibatch[i][3]\n","            done.append(minibatch[i][4])\n","\n","        # do batch prediction to save speed\n","        # predict Q-values for starting state using the main network\n","        target = self.model.predict(state)\n","        # predict best action in ending state using the main network\n","        target_next = self.model.predict(next_state)\n","        # predict Q-values for ending state using the target network\n","        target_val = self.target_model.predict(next_state)\n","\n","        for i in range(len(minibatch)):\n","            # correction on the Q value for the action used\n","            if done[i]:\n","                target[i][action[i]] = reward[i]\n","            else:\n","                if self.ddqn: # Double - DQN\n","                    # current Q Network selects the action\n","                    # a'_max = argmax_a' Q(s', a')\n","                    a = np.argmax(target_next[i])\n","                    # target Q Network evaluates the action\n","                    # Q_max = Q_target(s', a'_max)\n","                    target[i][action[i]] = reward[i] + self.gamma * (target_val[i][a])   \n","                else: # Standard - DQN\n","                    # DQN chooses the max Q value among next actions\n","                    # selection and evaluation of action is on the target Q Network\n","                    # Q_max = max_a' Q_target(s', a')\n","                    target[i][action[i]] = reward[i] + self.gamma * (np.amax(target_next[i]))\n","\n","        # Train the Neural Network with batches\n","        self.model.fit(state, target, batch_size=self.batch_size, verbose=0)\n","\n","    def load(self, name):\n","        self.model = load_model(name)\n","\n","    def save(self, name):\n","        self.model.save(name)\n","\n","    pylab.figure(figsize=(18, 9))\n","    def PlotModel(self, score, episode):\n","        self.scores.append(score)\n","        self.episodes.append(episode)\n","        self.average.append(sum(self.scores[-50:]) / len(self.scores[-50:]))\n","        pylab.plot(self.episodes, self.average, 'r')\n","        pylab.plot(self.episodes, self.scores, 'b')\n","        pylab.ylabel('Score', fontsize=18)\n","        pylab.xlabel('Steps', fontsize=18)\n","        dqn = 'DQN_'\n","        softupdate = ''\n","        dueling = ''\n","        if self.ddqn: dqn = 'DDQN_'\n","        if self.Soft_Update: softupdate = '_soft'\n","        if self.dueling: dueling = '_Dueling'\n","        try:\n","            pylab.savefig(dqn+self.env_name+softupdate+dueling+\".png\")\n","        except OSError:\n","            pass\n","\n","        return str(self.average[-1])[:5]\n","    \n","    def run(self):\n","        for e in range(self.EPISODES):\n","            state = self.env.reset()\n","            state = np.reshape(state, [1, self.state_size])\n","            done = False\n","            i = 0\n","            while not done:\n","                #self.env.render()\n","                action = self.act(state)\n","                next_state, reward, done, _ = self.env.step(action)\n","                next_state = np.reshape(next_state, [1, self.state_size])\n","                if not done or i == self.env._max_episode_steps-1:\n","                    reward = reward\n","                else:\n","                    reward = -100\n","                self.remember(state, action, reward, next_state, done)\n","                state = next_state\n","                i += 1\n","                if done:\n","                    # every step update target model\n","                    self.update_target_model()\n","                    \n","                    # every episode, plot the result\n","                    average = self.PlotModel(i, e)\n","                     \n","                    print(\"episode: {}/{}, score: {}, e: {:.2}, average: {}\".format(e, self.EPISODES, i, self.epsilon, average))\n","                    if i == self.env._max_episode_steps:\n","                        #print(\"Saving trained model as\", self.Model_name)\n","                        #self.save(self.Model_name)\n","                        break\n","                self.replay()\n","\n","    def test(self):\n","        self.load('ddqn.h5')\n","        for e in range(self.EPISODES):\n","            state = self.env.reset()\n","            state = np.reshape(state, [1, self.state_size])\n","            done = False\n","            i = 0\n","            while not done:\n","                self.env.render()\n","                action = np.argmax(self.model.predict(state))\n","                next_state, reward, done, _ = self.env.step(action)\n","                state = np.reshape(next_state, [1, self.state_size])\n","                i += 1\n","                if done:\n","                    print(\"episode: {}/{}, score: {}\".format(e, self.EPISODES, i))\n","                    break\n","\n"],"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1296x648 with 0 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"SaPIZKNmvlH8","outputId":"6df6eea3-c278-4f7d-82ce-de4ea86e8e9a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["if __name__ == \"__main__\":\n","    env_name = 'CartPole-v1'\n","    agent = DQNAgent(env_name)\n","    agent.run()\n","    #agent.test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------Double DQN--------\n","Model: \"CartPoleDuelingDDQNmodel\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 4)]          0                                            \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 512)          2560        input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          131328      dense[0][0]                      \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 64)           16448       dense_1[0][0]                    \n","__________________________________________________________________________________________________\n","dense_3 (Dense)                 (None, 1)            65          dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_4 (Dense)                 (None, 2)            130         dense_2[0][0]                    \n","__________________________________________________________________________________________________\n","lambda (Lambda)                 (None, 1)            0           dense_3[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_1 (Lambda)               (None, 2)            0           dense_4[0][0]                    \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 2)            0           lambda[0][0]                     \n","                                                                 lambda_1[0][0]                   \n","==================================================================================================\n","Total params: 150,531\n","Trainable params: 150,531\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","Model: \"CartPoleDuelingDDQNmodel\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 4)]          0                                            \n","__________________________________________________________________________________________________\n","dense_5 (Dense)                 (None, 512)          2560        input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dense_6 (Dense)                 (None, 256)          131328      dense_5[0][0]                    \n","__________________________________________________________________________________________________\n","dense_7 (Dense)                 (None, 64)           16448       dense_6[0][0]                    \n","__________________________________________________________________________________________________\n","dense_8 (Dense)                 (None, 1)            65          dense_7[0][0]                    \n","__________________________________________________________________________________________________\n","dense_9 (Dense)                 (None, 2)            130         dense_7[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_2 (Lambda)               (None, 1)            0           dense_8[0][0]                    \n","__________________________________________________________________________________________________\n","lambda_3 (Lambda)               (None, 2)            0           dense_9[0][0]                    \n","__________________________________________________________________________________________________\n","add_1 (Add)                     (None, 2)            0           lambda_2[0][0]                   \n","                                                                 lambda_3[0][0]                   \n","==================================================================================================\n","Total params: 150,531\n","Trainable params: 150,531\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","episode: 0/10000, score: 20, e: 1.0, average: 20.0\n","episode: 1/10000, score: 18, e: 1.0, average: 19.0\n","episode: 2/10000, score: 41, e: 1.0, average: 26.33\n","episode: 3/10000, score: 25, e: 1.0, average: 26.0\n","episode: 4/10000, score: 16, e: 1.0, average: 24.0\n","episode: 5/10000, score: 37, e: 1.0, average: 26.16\n","episode: 6/10000, score: 32, e: 1.0, average: 27.0\n","episode: 7/10000, score: 21, e: 1.0, average: 26.25\n","episode: 8/10000, score: 24, e: 1.0, average: 26.0\n","episode: 9/10000, score: 51, e: 1.0, average: 28.5\n","episode: 10/10000, score: 17, e: 1.0, average: 27.45\n","episode: 11/10000, score: 54, e: 1.0, average: 29.66\n","episode: 12/10000, score: 14, e: 1.0, average: 28.46\n","episode: 13/10000, score: 15, e: 1.0, average: 27.5\n","episode: 14/10000, score: 32, e: 1.0, average: 27.8\n","episode: 15/10000, score: 9, e: 1.0, average: 26.62\n","episode: 16/10000, score: 13, e: 1.0, average: 25.82\n","episode: 17/10000, score: 31, e: 1.0, average: 26.11\n","episode: 18/10000, score: 12, e: 1.0, average: 25.36\n","episode: 19/10000, score: 21, e: 1.0, average: 25.15\n","episode: 20/10000, score: 24, e: 1.0, average: 25.09\n","episode: 21/10000, score: 17, e: 1.0, average: 24.72\n","episode: 22/10000, score: 47, e: 1.0, average: 25.69\n","episode: 23/10000, score: 27, e: 1.0, average: 25.75\n","episode: 24/10000, score: 32, e: 1.0, average: 26.0\n","episode: 25/10000, score: 32, e: 1.0, average: 26.23\n","episode: 26/10000, score: 33, e: 1.0, average: 26.48\n","episode: 27/10000, score: 14, e: 1.0, average: 26.03\n","episode: 28/10000, score: 33, e: 1.0, average: 26.27\n","episode: 29/10000, score: 9, e: 1.0, average: 25.7\n","episode: 30/10000, score: 29, e: 1.0, average: 25.80\n","episode: 31/10000, score: 10, e: 1.0, average: 25.31\n","episode: 32/10000, score: 15, e: 1.0, average: 25.0\n","episode: 33/10000, score: 18, e: 1.0, average: 24.79\n","episode: 34/10000, score: 16, e: 1.0, average: 24.54\n","episode: 35/10000, score: 12, e: 1.0, average: 24.19\n","episode: 36/10000, score: 16, e: 1.0, average: 23.97\n","episode: 37/10000, score: 28, e: 1.0, average: 24.07\n","episode: 38/10000, score: 26, e: 1.0, average: 24.12\n","episode: 39/10000, score: 10, e: 1.0, average: 23.77\n","episode: 40/10000, score: 45, e: 1.0, average: 24.29\n","episode: 41/10000, score: 11, e: 0.99, average: 23.97\n","episode: 42/10000, score: 19, e: 0.97, average: 23.86\n","episode: 43/10000, score: 14, e: 0.96, average: 23.63\n","episode: 44/10000, score: 22, e: 0.94, average: 23.6\n","episode: 45/10000, score: 13, e: 0.93, average: 23.36\n","episode: 46/10000, score: 22, e: 0.91, average: 23.34\n","episode: 47/10000, score: 42, e: 0.87, average: 23.72\n","episode: 48/10000, score: 23, e: 0.85, average: 23.71\n","episode: 49/10000, score: 13, e: 0.84, average: 23.5\n","episode: 50/10000, score: 42, e: 0.8, average: 23.94\n","episode: 51/10000, score: 22, e: 0.79, average: 24.02\n","episode: 52/10000, score: 54, e: 0.75, average: 24.28\n","episode: 53/10000, score: 14, e: 0.74, average: 24.06\n","episode: 54/10000, score: 76, e: 0.68, average: 25.26\n","episode: 55/10000, score: 17, e: 0.67, average: 24.86\n","episode: 56/10000, score: 84, e: 0.62, average: 25.9\n","episode: 57/10000, score: 30, e: 0.6, average: 26.08\n","episode: 58/10000, score: 166, e: 0.51, average: 28.92\n","episode: 59/10000, score: 173, e: 0.43, average: 31.36\n","episode: 60/10000, score: 317, e: 0.31, average: 37.36\n","episode: 61/10000, score: 228, e: 0.25, average: 40.84\n","episode: 62/10000, score: 282, e: 0.19, average: 46.2\n","episode: 63/10000, score: 275, e: 0.14, average: 51.4\n","episode: 64/10000, score: 222, e: 0.11, average: 55.2\n","episode: 65/10000, score: 450, e: 0.072, average: 64.02\n","episode: 66/10000, score: 202, e: 0.059, average: 67.8\n","episode: 67/10000, score: 216, e: 0.048, average: 71.5\n","episode: 68/10000, score: 217, e: 0.038, average: 75.6\n","episode: 69/10000, score: 184, e: 0.032, average: 78.86\n","episode: 70/10000, score: 151, e: 0.027, average: 81.4\n","episode: 71/10000, score: 160, e: 0.023, average: 84.26\n","episode: 72/10000, score: 182, e: 0.019, average: 86.96\n","episode: 73/10000, score: 257, e: 0.015, average: 91.56\n","episode: 74/10000, score: 183, e: 0.013, average: 94.58\n","episode: 75/10000, score: 371, e: 0.01, average: 101.3\n","episode: 76/10000, score: 340, e: 0.01, average: 107.5\n","episode: 77/10000, score: 224, e: 0.01, average: 111.7\n","episode: 78/10000, score: 45, e: 0.01, average: 111.9\n","episode: 79/10000, score: 108, e: 0.01, average: 113.9\n","episode: 80/10000, score: 34, e: 0.01, average: 114.0\n","episode: 81/10000, score: 30, e: 0.01, average: 114.4\n","episode: 82/10000, score: 40, e: 0.01, average: 114.9\n","episode: 83/10000, score: 216, e: 0.01, average: 118.8\n","episode: 84/10000, score: 251, e: 0.01, average: 123.5\n","episode: 85/10000, score: 323, e: 0.01, average: 129.8\n","episode: 86/10000, score: 235, e: 0.01, average: 134.1\n","episode: 87/10000, score: 223, e: 0.01, average: 138.0\n","episode: 88/10000, score: 176, e: 0.01, average: 141.0\n","episode: 89/10000, score: 216, e: 0.01, average: 145.2\n","episode: 90/10000, score: 506, e: 0.01, average: 154.4\n","episode: 91/10000, score: 939, e: 0.01, average: 172.9\n","episode: 92/10000, score: 369, e: 0.01, average: 179.9\n","episode: 93/10000, score: 179, e: 0.01, average: 183.2\n","episode: 94/10000, score: 246, e: 0.01, average: 187.7\n","episode: 95/10000, score: 300, e: 0.01, average: 193.5\n","episode: 96/10000, score: 453, e: 0.01, average: 202.1\n","episode: 97/10000, score: 85, e: 0.01, average: 202.9\n","episode: 98/10000, score: 231, e: 0.01, average: 207.1\n","episode: 99/10000, score: 39, e: 0.01, average: 207.6\n","episode: 100/10000, score: 105, e: 0.01, average: 208.9\n","episode: 101/10000, score: 143, e: 0.01, average: 211.3\n","episode: 102/10000, score: 332, e: 0.01, average: 216.9\n","episode: 103/10000, score: 193, e: 0.01, average: 220.4\n","episode: 104/10000, score: 323, e: 0.01, average: 225.4\n","episode: 105/10000, score: 137, e: 0.01, average: 227.8\n","episode: 106/10000, score: 118, e: 0.01, average: 228.5\n","episode: 107/10000, score: 11, e: 0.01, average: 228.1\n","episode: 108/10000, score: 131, e: 0.01, average: 227.4\n","episode: 109/10000, score: 211, e: 0.01, average: 228.1\n","episode: 110/10000, score: 162, e: 0.01, average: 225.0\n","episode: 111/10000, score: 221, e: 0.01, average: 224.9\n","episode: 112/10000, score: 1459, e: 0.01, average: 248.4\n","episode: 113/10000, score: 211, e: 0.01, average: 247.2\n","episode: 114/10000, score: 189, e: 0.01, average: 246.5\n","episode: 115/10000, score: 880, e: 0.01, average: 255.1\n","episode: 116/10000, score: 215, e: 0.01, average: 255.4\n","episode: 117/10000, score: 175, e: 0.01, average: 254.5\n","episode: 118/10000, score: 47, e: 0.01, average: 251.1\n","episode: 119/10000, score: 46, e: 0.01, average: 248.4\n","episode: 120/10000, score: 41, e: 0.01, average: 246.2\n","episode: 121/10000, score: 41, e: 0.01, average: 243.8\n","episode: 122/10000, score: 42, e: 0.01, average: 241.0\n","episode: 123/10000, score: 24, e: 0.01, average: 236.3\n","episode: 124/10000, score: 70, e: 0.01, average: 234.1\n","episode: 125/10000, score: 72, e: 0.01, average: 228.1\n","episode: 126/10000, score: 167, e: 0.01, average: 224.6\n","episode: 127/10000, score: 213, e: 0.01, average: 224.4\n","episode: 128/10000, score: 195, e: 0.01, average: 227.4\n","episode: 129/10000, score: 184, e: 0.01, average: 228.9\n","episode: 130/10000, score: 174, e: 0.01, average: 231.7\n","episode: 131/10000, score: 174, e: 0.01, average: 234.6\n","episode: 132/10000, score: 177, e: 0.01, average: 237.4\n","episode: 133/10000, score: 182, e: 0.01, average: 236.7\n","episode: 134/10000, score: 189, e: 0.01, average: 235.4\n","episode: 135/10000, score: 200, e: 0.01, average: 233.0\n","episode: 136/10000, score: 166, e: 0.01, average: 231.6\n","episode: 137/10000, score: 187, e: 0.01, average: 230.9\n","episode: 138/10000, score: 209, e: 0.01, average: 231.5\n","episode: 139/10000, score: 179, e: 0.01, average: 230.8\n","episode: 140/10000, score: 184, e: 0.01, average: 224.4\n","episode: 141/10000, score: 205, e: 0.01, average: 209.7\n","episode: 142/10000, score: 177, e: 0.01, average: 205.8\n","episode: 143/10000, score: 162, e: 0.01, average: 205.5\n","episode: 144/10000, score: 179, e: 0.01, average: 204.2\n","episode: 145/10000, score: 165, e: 0.01, average: 201.5\n","episode: 146/10000, score: 199, e: 0.01, average: 196.4\n","episode: 147/10000, score: 139, e: 0.01, average: 197.5\n","episode: 148/10000, score: 159, e: 0.01, average: 196.0\n","episode: 149/10000, score: 141, e: 0.01, average: 198.1\n","episode: 150/10000, score: 157, e: 0.01, average: 199.1\n","episode: 151/10000, score: 173, e: 0.01, average: 199.7\n","episode: 152/10000, score: 147, e: 0.01, average: 196.0\n","episode: 153/10000, score: 168, e: 0.01, average: 195.5\n","episode: 154/10000, score: 193, e: 0.01, average: 192.9\n","episode: 155/10000, score: 140, e: 0.01, average: 193.0\n","episode: 156/10000, score: 150, e: 0.01, average: 193.6\n","episode: 157/10000, score: 210, e: 0.01, average: 197.6\n","episode: 158/10000, score: 224, e: 0.01, average: 199.4\n","episode: 159/10000, score: 369, e: 0.01, average: 202.6\n","episode: 160/10000, score: 43, e: 0.01, average: 200.2\n","episode: 161/10000, score: 164, e: 0.01, average: 199.1\n","episode: 162/10000, score: 271, e: 0.01, average: 175.3\n","episode: 163/10000, score: 23, e: 0.01, average: 171.6\n","episode: 164/10000, score: 15, e: 0.01, average: 168.1\n","episode: 165/10000, score: 15, e: 0.01, average: 150.8\n","episode: 166/10000, score: 16, e: 0.01, average: 146.8\n","episode: 167/10000, score: 197, e: 0.01, average: 147.2\n","episode: 168/10000, score: 17, e: 0.01, average: 146.6\n","episode: 169/10000, score: 13, e: 0.01, average: 146.0\n","episode: 170/10000, score: 14, e: 0.01, average: 145.4\n","episode: 171/10000, score: 173, e: 0.01, average: 148.1\n","episode: 172/10000, score: 254, e: 0.01, average: 152.3\n","episode: 173/10000, score: 255, e: 0.01, average: 156.9\n","episode: 174/10000, score: 194, e: 0.01, average: 159.4\n","episode: 175/10000, score: 181, e: 0.01, average: 161.6\n","episode: 176/10000, score: 206, e: 0.01, average: 162.4\n","episode: 177/10000, score: 172, e: 0.01, average: 161.6\n","episode: 178/10000, score: 160, e: 0.01, average: 160.9\n","episode: 179/10000, score: 191, e: 0.01, average: 161.0\n","episode: 180/10000, score: 215, e: 0.01, average: 161.8\n","episode: 181/10000, score: 222, e: 0.01, average: 162.8\n","episode: 182/10000, score: 231, e: 0.01, average: 163.9\n","episode: 183/10000, score: 200, e: 0.01, average: 164.2\n","episode: 184/10000, score: 181, e: 0.01, average: 164.1\n","episode: 185/10000, score: 193, e: 0.01, average: 163.9\n","episode: 186/10000, score: 183, e: 0.01, average: 164.3\n","episode: 187/10000, score: 256, e: 0.01, average: 165.6\n","episode: 188/10000, score: 243, e: 0.01, average: 166.3\n","episode: 189/10000, score: 507, e: 0.01, average: 172.9\n","episode: 190/10000, score: 290, e: 0.01, average: 175.0\n","episode: 191/10000, score: 298, e: 0.01, average: 176.9\n","episode: 192/10000, score: 778, e: 0.01, average: 188.9\n","episode: 193/10000, score: 212, e: 0.01, average: 189.9\n","episode: 194/10000, score: 279, e: 0.01, average: 191.9\n","episode: 195/10000, score: 356, e: 0.01, average: 195.7\n","episode: 196/10000, score: 247, e: 0.01, average: 196.7\n","episode: 197/10000, score: 209, e: 0.01, average: 198.1\n","episode: 198/10000, score: 325, e: 0.01, average: 201.4\n","episode: 199/10000, score: 406, e: 0.01, average: 206.7\n","episode: 200/10000, score: 263, e: 0.01, average: 208.8\n","episode: 201/10000, score: 322, e: 0.01, average: 211.8\n","episode: 202/10000, score: 577, e: 0.01, average: 220.4\n","episode: 203/10000, score: 205, e: 0.01, average: 221.1\n","episode: 204/10000, score: 272, e: 0.01, average: 222.7\n","episode: 205/10000, score: 769, e: 0.01, average: 235.3\n","episode: 206/10000, score: 592, e: 0.01, average: 244.1\n","episode: 207/10000, score: 237, e: 0.01, average: 244.7\n","episode: 208/10000, score: 217, e: 0.01, average: 244.5\n","episode: 209/10000, score: 223, e: 0.01, average: 241.6\n","episode: 210/10000, score: 334, e: 0.01, average: 247.4\n","episode: 211/10000, score: 304, e: 0.01, average: 250.2\n","episode: 212/10000, score: 286, e: 0.01, average: 250.5\n","episode: 213/10000, score: 271, e: 0.01, average: 255.5\n","episode: 214/10000, score: 216, e: 0.01, average: 259.5\n","episode: 215/10000, score: 295, e: 0.01, average: 265.1\n","episode: 216/10000, score: 26, e: 0.01, average: 265.3\n","episode: 217/10000, score: 262, e: 0.01, average: 266.6\n","episode: 218/10000, score: 246, e: 0.01, average: 271.2\n","episode: 219/10000, score: 293, e: 0.01, average: 276.8\n","episode: 220/10000, score: 244, e: 0.01, average: 281.4\n","episode: 221/10000, score: 222, e: 0.01, average: 282.4\n","episode: 222/10000, score: 194, e: 0.01, average: 281.2\n","episode: 223/10000, score: 255, e: 0.01, average: 281.2\n","episode: 224/10000, score: 194, e: 0.01, average: 281.2\n","episode: 225/10000, score: 248, e: 0.01, average: 282.5\n","episode: 226/10000, score: 263, e: 0.01, average: 283.6\n","episode: 227/10000, score: 320, e: 0.01, average: 286.6\n","episode: 228/10000, score: 318, e: 0.01, average: 289.8\n","episode: 229/10000, score: 227, e: 0.01, average: 290.5\n","episode: 230/10000, score: 223, e: 0.01, average: 290.6\n","episode: 231/10000, score: 240, e: 0.01, average: 291.0\n","episode: 232/10000, score: 227, e: 0.01, average: 290.9\n","episode: 233/10000, score: 181, e: 0.01, average: 290.5\n","episode: 234/10000, score: 184, e: 0.01, average: 290.6\n","episode: 235/10000, score: 204, e: 0.01, average: 290.8\n","episode: 236/10000, score: 194, e: 0.01, average: 291.0\n","episode: 237/10000, score: 182, e: 0.01, average: 289.6\n","episode: 238/10000, score: 163, e: 0.01, average: 288.0\n","episode: 239/10000, score: 174, e: 0.01, average: 281.3\n","episode: 240/10000, score: 171, e: 0.01, average: 278.9\n","episode: 241/10000, score: 12, e: 0.01, average: 273.2\n","episode: 242/10000, score: 11, e: 0.01, average: 257.9\n","episode: 243/10000, score: 11, e: 0.01, average: 253.8\n","episode: 244/10000, score: 112, e: 0.01, average: 250.5\n","episode: 245/10000, score: 146, e: 0.01, average: 246.3\n","episode: 246/10000, score: 148, e: 0.01, average: 244.3\n","episode: 247/10000, score: 204, e: 0.01, average: 244.2\n","episode: 248/10000, score: 179, e: 0.01, average: 241.3\n","episode: 249/10000, score: 171, e: 0.01, average: 236.6\n","episode: 250/10000, score: 187, e: 0.01, average: 235.1\n","episode: 251/10000, score: 303, e: 0.01, average: 234.7\n","episode: 252/10000, score: 286, e: 0.01, average: 228.9\n","episode: 253/10000, score: 256, e: 0.01, average: 229.9\n","episode: 254/10000, score: 137, e: 0.01, average: 227.2\n","episode: 255/10000, score: 163, e: 0.01, average: 215.1\n","episode: 256/10000, score: 161, e: 0.01, average: 206.5\n","episode: 257/10000, score: 205, e: 0.01, average: 205.8\n","episode: 258/10000, score: 252, e: 0.01, average: 206.5\n","episode: 259/10000, score: 223, e: 0.01, average: 206.5\n","episode: 260/10000, score: 4000, e: 0.01, average: 279.8\n","episode: 261/10000, score: 1229, e: 0.01, average: 298.3\n","episode: 262/10000, score: 543, e: 0.01, average: 303.5\n","episode: 263/10000, score: 325, e: 0.01, average: 304.6\n","episode: 264/10000, score: 410, e: 0.01, average: 308.4\n","episode: 265/10000, score: 365, e: 0.01, average: 309.8\n","episode: 266/10000, score: 220, e: 0.01, average: 313.7\n","episode: 267/10000, score: 609, e: 0.01, average: 320.7\n","episode: 268/10000, score: 243, e: 0.01, average: 320.6\n","episode: 269/10000, score: 161, e: 0.01, average: 318.0\n","episode: 270/10000, score: 104, e: 0.01, average: 315.2\n","episode: 271/10000, score: 136, e: 0.01, average: 313.4\n","episode: 272/10000, score: 87, e: 0.01, average: 311.3\n","episode: 273/10000, score: 50, e: 0.01, average: 307.2\n","episode: 274/10000, score: 87, e: 0.01, average: 305.1\n","episode: 275/10000, score: 282, e: 0.01, average: 305.7\n","episode: 276/10000, score: 435, e: 0.01, average: 309.2\n","episode: 277/10000, score: 121, e: 0.01, average: 305.2\n","episode: 278/10000, score: 226, e: 0.01, average: 303.4\n","episode: 279/10000, score: 171, e: 0.01, average: 302.2\n","episode: 280/10000, score: 712, e: 0.01, average: 312.0\n","episode: 281/10000, score: 134, e: 0.01, average: 309.9\n","episode: 282/10000, score: 149, e: 0.01, average: 308.3\n","episode: 283/10000, score: 143, e: 0.01, average: 307.6\n","episode: 284/10000, score: 149, e: 0.01, average: 306.9\n","episode: 285/10000, score: 146, e: 0.01, average: 305.7\n","episode: 286/10000, score: 141, e: 0.01, average: 304.7\n","episode: 287/10000, score: 162, e: 0.01, average: 304.3\n","episode: 288/10000, score: 202, e: 0.01, average: 305.0\n","episode: 289/10000, score: 311, e: 0.01, average: 307.8\n","episode: 290/10000, score: 186, e: 0.01, average: 308.1\n","episode: 291/10000, score: 209, e: 0.01, average: 312.0\n","episode: 292/10000, score: 722, e: 0.01, average: 326.2\n","episode: 293/10000, score: 388, e: 0.01, average: 333.8\n","episode: 294/10000, score: 137, e: 0.01, average: 334.3\n","episode: 295/10000, score: 151, e: 0.01, average: 334.4\n","episode: 296/10000, score: 188, e: 0.01, average: 335.2\n","episode: 297/10000, score: 176, e: 0.01, average: 334.6\n","episode: 298/10000, score: 176, e: 0.01, average: 334.6\n","episode: 299/10000, score: 3522, e: 0.01, average: 401.6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jf5rCM5cEKqr"},"source":["agent.test()\n"],"execution_count":null,"outputs":[]}]}