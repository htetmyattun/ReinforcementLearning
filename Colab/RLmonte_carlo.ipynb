{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RLmonte_carlo.ipynb","provenance":[],"authorship_tag":"ABX9TyNG3Z5QneQszbgE/10BtNp0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"9HQQxGYvOFTI","executionInfo":{"status":"ok","timestamp":1601893838624,"user_tz":-480,"elapsed":1033,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["import gym\n","import numpy as np\n","import operator\n","from IPython.display import clear_output\n","from time import sleep\n","import random\n","import itertools\n","import tqdm\n","\n","tqdm.monitor_interval = 10"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"DSNwpP4ROseA","executionInfo":{"status":"ok","timestamp":1601893838629,"user_tz":-480,"elapsed":1026,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["def create_random_policy(env):\n","     policy = {}\n","     for key in range(0, env.observation_space.n):\n","          current_end = 0\n","          p = {}\n","          for action in range(0, env.action_space.n):\n","               p[action] = 1 / env.action_space.n\n","          policy[key] = p\n","     return policy"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"V5CZMLRdOvjs","executionInfo":{"status":"ok","timestamp":1601893838631,"user_tz":-480,"elapsed":1021,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["def create_state_action_dictionary(env, policy):\n","    Q = {}\n","    for key in policy.keys():\n","         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n","    return Q"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLw0tXD1O682","executionInfo":{"status":"ok","timestamp":1601893838632,"user_tz":-480,"elapsed":1017,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["def run_game(env, policy, display=True):\n","     env.reset()\n","     episode = []\n","     finished = False\n","\n","     while not finished:\n","          s = env.env.s\n","          if display:\n","               clear_output(True)\n","               env.render()\n","               sleep(0)\n","\n","          timestep = []\n","          timestep.append(s)\n","          n = random.uniform(0, sum(policy[s].values()))\n","          #print(\"sum\",sum(policy[s].values()))\n","          #print(\"nnn\",n)\n","          top_range = 0\n","          for prob in policy[s].items():\n","                top_range += prob[1]\n","                #print(n,top_range)\n","                if n < top_range:\n","                      action = prob[0]\n","                      #print(\"break\")\n","                      break \n","          state, reward, finished, info = env.step(action)\n","          timestep.append(action)\n","          timestep.append(reward)\n","          episode.append(timestep)\n","\n","     if display:\n","          clear_output(True)\n","          env.render()\n","          sleep(1)\n","     return episode"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"HU3o3w83O9rJ","executionInfo":{"status":"ok","timestamp":1601893838635,"user_tz":-480,"elapsed":1014,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["def test_policy(policy, env):\n","      wins = 0\n","      r = 100\n","      for i in range(r):\n","            w = run_game(env, policy, display=False)[-1][-1]\n","            if w == 1:\n","                  wins += 1\n","      return wins / r"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"vb0ookpvPAaW","executionInfo":{"status":"ok","timestamp":1601893838637,"user_tz":-480,"elapsed":1011,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}}},"source":["def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n","    if not policy:\n","        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n","    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n","    returns = {} # 3.\n","    \n","    for episode in range(episodes): # Looping through episodes\n","        G = 0 # Store cumulative reward in G (initialized at 0)\n","        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n","        \n","        # for loop through reversed indices of episode array. \n","        # The logic behind it being reversed is that the eventual reward would be at the end. \n","        # So we have to go back from the last timestep to the first one propagating result from the future.\n","        \n","        for i in reversed(range(0, len(episode))):   \n","            s_t, a_t, r_t = episode[i] \n","            state_action = (s_t, a_t)\n","            G += r_t # Increment total reward by reward on current timestep\n","            #print(\"state action \",state_action in [(x[0], x[1]) for x in episode[0:i]])\n","            #print(\"\\n\")\n","            #for x in episode[0:i]:\n","             #   print(x[0],\" \",x[1])\n","            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n","                if returns.get(state_action):\n","                    returns[state_action].append(G)\n","                else:\n","                    returns[state_action] = [G]   \n","                    \n","                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n","                \n","                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n","                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n","                max_Q = random.choice(indices)\n","                \n","                A_star = max_Q # 14.\n","                \n","                for a in policy[s_t].items(): # Update action probability for s_t in policy\n","                    if a[0] == A_star:\n","                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n","                    else:\n","                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n","\n","    return policy"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"hyd7PDivPEHq"},"source":["env=gym.make('FrozenLake-v0')\n","policy=monte_carlo_e_soft(env,episodes=1000000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5QDB4QKPUnw","executionInfo":{"status":"error","timestamp":1601914815608,"user_tz":-480,"elapsed":2032,"user":{"displayName":"altotech aitraining","photoUrl":"","userId":"00866022040909509342"}},"outputId":"6f4d178d-b229-4e90-9dbd-0e4c4796bb60","colab":{"base_uri":"https://localhost:8080/","height":174}},"source":["test_policy(policy,env)"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-741e8e091c2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'test_policy' is not defined"]}]}]}